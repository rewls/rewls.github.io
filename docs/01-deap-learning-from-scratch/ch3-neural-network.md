---
layout: default
title: Ch3 ì‹ ê²½ë§
parent: Deap Learning from Scratch
nav_order: 3
mathjax: true
mermaid: true
---

# Ch3 ì‹ ê²½ë§
{: .no_toc }

<details open markdown="block">
  <summary>
    Toggle
  </summary>

## Table of contents
{: .no_toc .text-delta }

- TOC
{:toc}
</details>

---

- ì• ì¥ì—ì„œëŠ” ì¸ê°„ì´ ì ì ˆí•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì •í–ˆì§€ë§Œ ì‹ ê²½ë§ì„ ì´ìš©í•˜ë©´ ë§¤ê°œë³€ìˆ˜ë¥¼ ë°ì´í„°ë¡œë¶€í„° ìë™ìœ¼ë¡œ í•©ìŠµí•  ìˆ˜ ìˆë‹¤.

- ì´ë²ˆ ì¥ì—ì„œëŠ” ì‹ ê²½ë§ì˜ ê°œìš”ë¥¼ ì„¤ëª…í•˜ê³ , ì‹ ê²½ë§ì´ ì…ë ¥ ë°ì´í„°ê°€ ë¬´ì—‡ì¸ì§€ ì‹ë³„í•˜ëŠ” ì²˜ë¦¬ ê³¼ì •ì„ ìì„¸íˆ ì•Œì•„ë³¸ë‹¤.

## 3.1 í¼ì…‰íŠ¸ë¡ ì—ì„œ ì‹ ê²½ë§ìœ¼ë¡œ

### 3.1.1 ì‹ ê²½ë§ì˜ ì˜ˆ

<div class="mermaid">
graph LR;
	x1(( )); x2(( )); s1(( )); s2(( )); s3(( )); y1(( )); y2(( ));
	x1 --> s1; x1 --> s2; x1 --> s3;
	x2 --> s1; x2 --> s2; x2 --> s3;
	s1 --> y1; s1 --> y2;
	s2 --> y1; s2 --> y2;
	s3 --> y1; s3 --> y2;
	subgraph input[ì…ë ¥ì¸µ, 0ì¸µ];
		x1; x2;
		end;
	subgraph hidden[ì€ë‹‰ì¸µ, 1ì¸µ];
		s1; s2; s3;
		end;
	subgraph out[ì¶œë ¥ì¸µ, 2ì¸µ];
		y1; y2;
		end;
</div>

- ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ì€ ì‚¬ëŒ ëˆˆì— ë³´ì´ì§€ ì•ŠëŠ”ë‹¤.

> ìœ„ì˜ ì‹ ê²½ë§ì€ ëª¨ë‘ 3ì¸µìœ¼ë¡œ êµ¬ì„±ë˜ì§€ë§Œ ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ” ì¸µì€ 2ê°œë¿ì´ê¸° ë•Œë¬¸ì— 2ì¸µ ì‹ ê²½ë§ìœ¼ë¡œ í•œë‹¤. ê²½ìš°ì— ë”°ë¼ 3ì¸µ ì‹ ê²½ë§ì´ë¼ í•˜ëŠ” ê²½ìš°ë„ ìˆìœ¼ë‹ˆ ì£¼ì˜

### 3.1.2 í¼ì…‰íŠ¸ë¡  ë³µìŠµ

<div class="mermaid">
graph LR;
	x1((x1)); x2((x2)); y((y));
	x1 --w1-->y;
	x2 --w2-->y;
</div>

<div>
$$
y = \begin{cases}
	0~(b + w_1x_1 + w_2x_2 \le 0)\\
	1~(b + w_1x_1 + w_2x_2 > 0)\\
	\end{cases}
$$
</div>

- $b$ëŠ” **í¸í–¥**ì„ ë‚˜íƒ€ë‚´ëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ, ë‰´ëŸ°ì´ ì–¼ë§ˆë‚˜ ì‰½ê²Œ í™œì„±í™”ë˜ëŠëƒë¥¼ ì œì–´

- $w_1$ê³¼ $w_2$ëŠ” ê° ì‹ í˜¸ì˜ **ê°€ì¤‘ì¹˜**ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë§¤ê°œë³€ìˆ˜ë¡œ, ê° ì‹ í˜¸ì˜ ì˜í–¥ë ¥ì„ ì œì–´

<div class="mermaid">
graph LR;
	x1((x1)); x2((x2)); 1((1)); y((y));
	1 --b--> y;
	x1 --w1--> y;
	x2 --w2--> y;
</div>

- í¸í–¥ì„ ëª…ì‹œí•œ í¼ì…‰íŠ¸ë¡  êµ¬ì¡°

- ê°€ì¤‘ì¹˜ê°€ $b$ì´ê³  ì…ë ¥ì´ 1ì¸ ë‰´ëŸ° ì¶”ê°€

<div>
$$
y = h(b + w_1x_1 + w_2x_2)
$$
</div>

<div>
$$
h(x) = \begin{cases}
	0~(x \le 0)\\
	1~(x > 0)\\
	\end{cases}
$$
</div>

- ì…ë ¥ ì‹ í˜¸ì˜ ì´í•©ì´ $h(x)$ë¼ëŠ” í•¨ìˆ˜ë¥¼ ê±°ì³ ë³€í™˜ë˜ì–´, ê·¸ ë³€í™˜ëœ ê°’ì´ $y$ì˜ ì¶œë ¥ì´ ëœë‹¤.

### 3.1.3 í™œì„±í™” í•¨ìˆ˜ì˜ ë“±ì¥

- ì…ë ¥ ì‹ í˜¸ì˜ ì´í•©ì„ ì¶œë ¥ ì‹ í˜¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì¼ë°˜ì ìœ¼ë¡œ **í™œì„±í™” í•¨ìˆ˜**$^{\text{activation function}}$ë¼ í•œë‹¤.

	- ì…ë ¥ ì‹ í˜¸ì˜ ì´í•©ì´ í™œì„±í™”ë¥¼ ì¼ìœ¼í‚¤ëŠ”ì§€ ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤.

<div>
$$
a = b + w_1x_1 + w_2x_2
$$
</div>

<div>
$$
y = h(a)
$$
</div>

<div class="mermaid">
graph LR;
	x1((x1)); x2((x2)); 1((1)); y(("a --h(x)--> y"));
	1 --b--> y;
	x1 --w1--> y;
	x2 --w2--> y;
</div>

- í™œì„±í™” í•¨ìˆ˜ ì²˜ë¦¬ ê³¼ì • ëª…ì‹œí•œ í¼ì…‰íŠ¸ë¡  êµ¬ì¡°

> - **ë‹¨ìˆœ í¼ì…‰íŠ¸ë¡ **ì€ ë‹¨ì¸µ ë„¤íŠ¸ì›Œí¬ì—ì„œ ê³„ë‹¨ í•¨ìˆ˜(ì„ê³„ê°’ì„ ê²½ê³„ë¡œ ì¶œë ¥ì´ ë°”ë€ŒëŠ” í•¨ìˆ˜)ë¥¼ í™œì„±í™” í•¨ìˆ˜ë¡œ ì‚¬ìš©í•œ ëª¨ë¸ì„ ê°€ë¦¬í‚¨ë‹¤.
> - **ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ **ì€ ì‹ ê²½ë§(ì—¬ëŸ¬ ì¸µìœ¼ë¡œ êµ¬ì„±ë˜ê³  ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ë“±ì˜ ë§¤ëˆí•œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë„¤íŠ¸ì›Œí¬)ì„ ê°€ë¦¬í‚¨ë‹¤.

---

## 3.2 í™œì„±í™” í•¨ìˆ˜

<div>
$$
h(x) = \begin{cases}
	0~(x \le 0)\\
	1~(x > 0)\\
	\end{cases}
$$
</div>

- ì„ê³„ê°’ì„ ê²½ê³„ë¡œ ì¶œë ¥ì´ ë°”ë€ŒëŠ” í•¨ìˆ˜ë¥¼ **ê³„ë‹¨ í•¨ìˆ˜**ë¼ê³  í•œë‹¤.

- í™œì„±í™” í•¨ìˆ˜ë¥¼ ê³„ë‹¨ í•¨ìˆ˜ì—ì„œ ë‹¤ë¥¸ í•¨ìˆ˜ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì´ ì‹ ê²½ë§ì˜ ì„¸ê³„ë¡œ ë‚˜ì•„ê°€ëŠ” ì—´ì‡ !

### 3.2.1 ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜

<div>
$$
h(x) = {1 \over 1 + exp(-x)}
$$
</div>

- $exp(-x)$ëŠ” $e^{-x}$ë¥¼ ëœ»í•œë‹¤.

- ì‹ ê²½ë§ì—ì„œëŠ” í™œì„±í™” í•¨ìˆ˜ë¡œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì´ìš©í•œë‹¤.

- í¼ì…‰íŠ¸ë¡ ê³¼ ì‹ ê²½ë§ì˜ ì£¼ëœ ì°¨ì´ëŠ” í™œì„±í™” í•¨ìˆ˜ ë¿ì´ë‹¤.

### 3.2.2 ê³„ë‹¨ í•¨ìˆ˜ êµ¬í˜„

```python
def step_function(x):
	if x > 0:
		return 1
	else:
		return 0
```

- ì´ êµ¬í˜„ì—ì„œ ì¸ìˆ˜ xëŠ” ì‹¤ìˆ˜(ë¶€ë™ì†Œìˆ˜ì )ë§Œ ë°›ì•„ë“¤ì´ê¸° ë•Œë¬¸ì— ë„˜íŒŒì´ ë°°ì—´ì„ ì¸ìˆ˜ë¡œ ë„£ì„ ìˆ˜ëŠ” ì—†ë‹¤.

```python
def step_function(x):
	y = x > 0
	return y.astype(np.int)
```

- ë„˜íŒŒì´ ë°°ì—´ë„ ì¸ìˆ˜ë¡œ ë°›ì„ ìˆ˜ ìˆë„ë¡ ìˆ˜ì •í•œ êµ¬í˜„

```python
# interpreter
import numpy as np
x = np.array([-1.0, 1.0, 2.0])
x
y = x > 0
y
y = y.astype(int)
y
```

- ë„˜íŒŒì´ ë°°ì—´ì— ë¶€ë“±í˜¸ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë©´ ë°°ì—´ì˜ ê°ê°ì— ë¶€ë“±í˜¸ ì—°ì‚°ì„ ìˆ˜í–‰í•œ bool ë°°ì—´ì´ ìƒì„±ëœë‹¤.

- `numpy.astype` : ë„˜íŒŒì´ ë°°ì—´ì˜ ìë£Œí˜•ì„ ë³€í™˜í•œë‹¤. ì¸ìˆ˜ë¡œ ë³€í™˜í•  ìë£Œí˜•ì„ ì§€ì •í•œë‹¤.

### 3.2.3 ê³„ë‹¨ í•¨ìˆ˜ì˜ ê·¸ë˜í”„

```python
%matplotlib tk
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
	return np.array(x>0, dtype=int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)	# yì¶•ì˜ ë²”ìœ„ ì§€ì •
plt.show()
```

### 3.2.4 ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ êµ¬í˜„í•˜ê¸°

```python
%matplotlib tk
import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
	return 1 / (1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)	# yì¶• ë²”ìœ„ ì§€ì •
plt.show()
```

- ì¸ìˆ˜ë¡œ ë„˜íŒŒì´ ë°°ì—´ì´ ì™€ë„ ë°”ë¥´ê²Œ ê³„ì‚°ëœë‹¤.

- ë¸Œë¡œë“œìºìŠ¤íŠ¸ë¥¼ í†µí•´ ë„˜íŒŒì´ ë°°ì—´ê³¼ ìŠ¤ì¹¼ë¼ê°’ì˜ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.

> ì‹œê·¸ëª¨ì´ë“œ(sigmoid)ë€ 'Sì ëª¨ì–‘'ì´ë¼ëŠ” ëœ»

### 3.2.5 ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ ê³„ë‹¨ í•¨ìˆ˜ ë¹„êµ

```python
%matplotlib tk
import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
	return 1 / (1 + np.exp(-x))

def step_function(x):
	return np.array(x>0, dtype=int)

x = np.arange(-5.0, 5.0, 0.1)
sig_y = sigmoid(x)
step_y = step_function(x)

plt.plot(x, sig_y, label="sigmoid")
plt.plot(x, step_y, linestyle="--", label="step")	# cos í•¨ìˆ˜ ì ì„ ìœ¼ë¡œ ê·¸ë¦¬ê¸°

plt.ylim(-0.1, 1.1)	# yì¶• ë²”ìœ„ ì§€ì •
plt.title('sigmoid & step')
plt.legend()
plt.show()
```

##### ì°¨ì´ì 
{: .no_toc }

- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ë¶€ë“œëŸ¬ìš´ ê³¡ì„ ì´ë©° ì…ë ¥ì— ë”°ë¼ ì¶œë ¥ì´ ì—°ì†ì ìœ¼ë¡œ ë³€í™”í•˜ê³ , ê³„ë‹¨ í•¨ìˆ˜ëŠ” 0ì„ ê²½ê³„ë¡œ ì¶œë ¥ì´ ê°‘ìê¸° ë°”ë€ë‹¤.

- ê³„ë‹¨ í•¨ìˆ˜ê°€ 0ê³¼ 1 ì¤‘ í•˜ë‚˜ì˜ ê°’ë§Œ ëŒë ¤ì£¼ëŠ” ë°˜ë©´ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ì‹¤ìˆ˜ë¥¼ ëŒë ¤ì¤€ë‹¤.

##### ê³µí†µì 
{: .no_toc }

- ì…ë ¥ì´ ì‘ì„ ë•Œì˜ ì¶œë ¥ì€ 0ì— ê°€ê¹ê³ , ì…ë ¥ì´ ì»¤ì§€ë§Œ ì¶œë ¥ì´ 1ì— ê°€ê¹Œì›Œì§€ëŠ” êµ¬ì¡°ì´ë‹¤.

- 0ì—ì„œ 1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ë¥¼ ì¶œë ¥í•œë‹¤.

### 3.2.6 ë¹„ì„ í˜• í•¨ìˆ˜

- ê³„ë‹¨ í•¨ìˆ˜ì™€ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ì¤‘ìš”í•œ ê³µí†µì ì€ **ë¹„ì„ í˜• í•¨ìˆ˜**ë¼ëŠ” ê²ƒ

> - í•¨ìˆ˜ì— ë¬´ì–¸ê°€ ì…ë ¥í–ˆì„ ë•Œ ì¶œë ¥ì´ ì…ë ¥ì˜ ìƒìˆ˜ë°°ë§Œí¼ ë³€í•˜ëŠ” í•¨ìˆ˜ë¥¼ **ì„ í˜• í•¨ìˆ˜**ë¼ê³  í•œë‹¤.
>	- $f(x) = ax + b$(a, bëŠ” ìƒìˆ˜)
> - **ë¹„ì„ í˜• í•¨ìˆ˜**ëŠ” ì„ í˜•ì´ ì•„ë‹Œ í•¨ìˆ˜

- ì‹ ê²½ë§ì—ì„œëŠ” í™œì„±í™” í•¨ìˆ˜ë¡œ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

- ì„ í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì‹ ê²½ë§ì˜ ì¸µì„ ê¹Šê²Œ í•˜ëŠ” ì˜ë¯¸ê°€ ì—†ì–´ì§€ê¸° ë•Œë¬¸

### 3.2.7 ReLU í•¨ìˆ˜

- ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ì‹ ê²½ë§ ë¶„ì•¼ì—ì„œ ì˜¤ë˜ì „ë¶€í„° ì´ìš©í•´ì™”ìœ¼ë‚˜, ìµœê·¼ì—ëŠ” **ReLU**$^{\text{Rectified Linear Unit, ë ë£¨}}$ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.

<div>
$$
h(x) = \begin{cases}
	x~(x > 0) \\
	0~(x \le 0)
	\end{cases}
$$
</div>
```python
%matplotlib tk
import numpy as np
import matplotlib.pylab as plt

def relu(x):
	return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)
plt.plot(x, y)
plt.ylim(-0.1, 5.1)	# yì¶•ì˜ ë²”ìœ„ ì§€ì •
plt.show()
```

- `numpy.maximum` : ë‘ ì…ë ¥ ì¤‘ í° ê°’ì„ ë°˜í™˜

---

## 3.3 ë‹¤ì°¨ì› ë°°ì—´ì˜ ê³„ì‚°

### 3.3.1 ë‹¤ì°¨ì› ë°°ì—´

```python
import numpy as np

A = np.array([1, 2, 3, 4])
print(A)
np.ndim(A)
A.shape
A.shape[0]
```

- `numpy.ndim` : ë°°ì—´ì˜ ì°¨ì› ìˆ˜ ë°˜í™˜

- `ndarray.shape` : ë°°ì—´ì˜ í˜•ìƒì„ íŠœí”Œë¡œ ë°˜í™˜

```python
B = np.array([[1,2], [3,4], [5,6]])
print(B)
np.ndim(B)
B.shape
```

- $n \times m$ ë°°ì—´ : 0ë²ˆì§¸ ì°¨ì›ì—ëŠ” ì›ì†Œê°€ nê°œ, 1ë²ˆì§¸ ì°¨ì›ì—ëŠ” ì›ì†Œê°€ mê°œ ìˆë‹¤ëŠ” ì˜ë¯¸

- 2ì°¨ì› ë°°ì—´ì€ **í–‰ë ¬**$^{\text{matrix}}$ì´ë¼ê³  ë¶€ë¥´ê³  ë°°ì—´ì˜ ê°€ë¡œ ë°©í–¥ì„ **í–‰**$^{\text{row}}$, ì„¸ë¡œ ë°©í–¥ì„ **ì—´**$^{\text{column}}$ì´ë¼ê³  í•œë‹¤.

### 3.3.2 í–‰ë ¬ì˜ ê³±

- ì™¼ìª½ í–‰ë ¬ì˜ í–‰(ê°€ë¡œ)ê³¼ ì˜¤ë¥¸ìª½ í–‰ë ¬ì˜ ì—´(ì„¸ë¡œ)ì„ ì›ì†Œë³„ë¡œ ê³±í•˜ê³  ê·¸ ê°’ë“¤ì„ ë”í•´ì„œ ê³„ì‚°í•œë‹¤.

- êµí™˜ë²•ì¹™ì€ ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤.

- ì²«ì§¸ í–‰ë ¬ì˜ 1ë²ˆì§¸ ì°¨ì›ì˜ ì›ì†Œìˆ˜(ì—´ ìˆ˜)ì™€ ë‘˜ì§¸ í–‰ë ¬ì˜ 0ë²ˆì§¸ ì°¨ì›ì˜ ì›ì†Œ ìˆ˜(í–‰ ìˆ˜)ê°€ ê°™ì•„ì•¼ í•œë‹¤.

<div>
$$
A~~~~~~~~~~~~B~~~~=~~~~C
$$
</div>

<div>
$$
m \times \cancel{n}~~~~\cancel{n} \times p~~~~~m \times p
$$
</div>

<div>
$$
A = \left(
	\begin{matrix}
		a_{11} & \cdots & a_{1n} \\
		\vdots & \ddots & \vdots \\
		a_{m1} & \cdots & a_{mn}
	\end{matrix}
	\right),~

B = \left(
	\begin{matrix}
		b_{11} & \cdots & b_{1p} \\
		\vdots & \ddots & \vdots \\
		b_{n1} & \cdots & b_{np}
	\end{matrix}
	\right)
$$
</div>

<div>
$$
C = AB
$$
</div>

<div>
$$
C = \left(
	\begin{matrix}
		c_{11} & \cdots & c_{1p} \\
		\vdots & \ddots & \vdots \\
		c_{1m} & \cdots & c_{mp}
	\end{matrix}
	\right)
$$
</div>

<div>
$$
c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj} = \sum^n_{k=1}a_{ik}b_{kj}
$$
</div>

<div>
$$
C = \left(
	\begin{matrix}
		a_{11}b_{11} + \cdots + a_{1n}b_{n1} & \cdots & a_{11}b_{1p} + \cdots a_{1n}b_{np} \\
		\vdots & \ddots & \vdots \\
		a_{m1}b_{11} + \cdots + a_{mn}b_{n1} & \cdots & a_{m1}b_{1p} + \cdots + a_{mn}b_{np}
	\end{matrix}
	\right)
$$
</div>

```python
A = np.array([[1,2], [3,4]])
A.shape
B = np.array([[5,6], [7,8]])
B.shape
np.dot(A, B)
```

- `numpy.dot` : í–‰ë ¬ì˜ ê³±

### 3.3.3 ì‹ ê²½ë§ì—ì„œì˜ í–‰ë ¬ ê³±

- í¸í–¥ê³¼ í™œì„±í™” í•¨ìˆ˜ ìƒëµ, ê°€ì¤‘ì¹˜ë§Œ ê°–ëŠ” ì‹ ëª…ë§ êµ¬í˜„

<div class="mermaid">
graph LR;
	x1((x1)); x2((x2)); y1((y1)); y2((y2)); y3((y3));
	x1 --1--> y1;
	x1 --3--> y2;
	x1 --5--> y3;
	x2 --2--> y1;
	x2 --4--> y2;
	x2 --6--> y3;
</div>

<div>
$$
W = \left(
	\begin{matrix}
		1 & 3 & 5 \\
		2 & 4 & 6
	\end{matrix}
	\right)
$$
</div>

<div>
$$
X~~~~~~~~~~W~~~=~~~~Y
$$
</div>

<div>
$$
\cancel2~~~~~~~\cancel2 \times 3~~~~~~~~~~3
$$
</div>

```python
# interpreter
X = np.array([1, 2])
X.shape
W = np.array([[1, 3, 5], [2, 4, 6]])
print(W)
W.shape
Y = np.dot(X, W)
print(Y)
```

- í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ í•œë²ˆì— ê³„ì‚°í•´ì£¼ëŠ” ê¸°ëŠ¥ì€ ì‹ ê²½ë§ êµ¬í˜„ì— ë§¤ìš° ì¤‘ìš”

---

## 3.4 3ì¸µ ì‹ ê²½ë§ êµ¬í˜„í•˜ê¸°

- ë„˜íŒŒì´ ë°°ì—´ì„ ì˜ ì“°ë©´ ì•„ì£¼ ì ì€ ì½”ë“œë§Œìœ¼ë¡œ ì‹ ê²½ë§ì˜ ìˆœë°©í–¥ ì²˜ë¦¬ë¥¼ ì™„ì„±í•  ìˆ˜ ìˆë‹¤.

<div class="mermaid">
graph LR;
	x1((x1)); x2((x2)); h11(( )); h12(( )); h13(( )); h21(( )); h22(( )); y1((y1)); y2((y2));
	x1 --> h11; x1 --> h12; x1 --> h13;
	x2 --> h11; x2 --> h12; x2 --> h13;
	h11 --> h21; h11 --> h22;
	h12 --> h21; h12 --> h22;
	h13 --> h21; h13 --> h22;
	h21 --> y1; h21 --> y2;
	h22 --> y1; h22 --> y2;
</div>

### 3.4.2 ê° ì¸µì˜ ì‹ í˜¸ ì „ë‹¬ êµ¬í˜„í•˜ê¸°

```python
import numpy as np

def sigmoid(x):
	return 1 / (1 + np.exp(-x))

def identity_function(x):
	return x

X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

print("(W1.shape) :", W1.shape)
print("(X.shape) :", X.shape)
print("(B1.shape) :", B1.shape)

A1 = np.dot(X, W1) + B1
Z1 = sigmoid(A1)

print("(A1) :", A1)
print("(Z1) :", Z1)

W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
B2 = np.array([0.1, 0.2])

print("(Z1.shape) :", Z1.shape)
print("(W2.shape) :", W2.shape)
print("(B2.shape) :", B2.shape)

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)

W3 = np.array([[0.1, 0.3], [0.2, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)
```

> ì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ëŠ” í’€ê³ ì í•˜ëŠ” ë¬¸ì œì˜ ì„±ì§ˆì— ë§ê²Œ ì •í•œë‹¤.
> ì¼ë°˜ì ìœ¼ë¡œ
> - íšŒê·€ì—ëŠ” í•­ë“±í•¨ìˆ˜
> - 2í´ë˜ìŠ¤ ë¶„ë¥˜ì—ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
> - ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì—ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜
> ë¥¼ ì‚¬ìš©í•œë‹¤.

### 3.4.3 êµ¬í˜„ ì •ë¦¬

- ì‹ ê²½ë§ êµ¬í˜„ì˜ ê´€ë¡€ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë§Œ ëŒ€ë¬¸ìë¡œ ì“°ê³  ê·¸ ì™¸ í¸í–¥ê³¼ ì¤‘ê°„ ê²°ê³¼ ë“±ì€ ëª¨ë‘ ì†Œë¬¸ìë¡œ ì“´ë‹¤.

```python
import numpy as np

def sigmoid(x):
	return 1 / (1 + np.exp(-x))

def identity_function(x):
	return x

def init_network():
	network = {}
	network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
	network['b1'] = np.array([0.1, 0.2, 0.3])
	network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
	network['b2'] = np.array([0.1, 0.2])
	network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
	network['b3'] = np.array([0.1, 0.2])

	return network

def forward(network, x):
	W1, W2, W3 = network['W1'], network['W2'], network['W3']
	b1, b2, b3 = network['b1'], network['b2'], network['b3']

	a1 = np.dot(x, W1) + b1
	z1 = sigmoid(a1)
	a2 = np.dot(z1, W2) + b2
	z2 = sigmoid(a2)
	a3 = np.dot(z2, W3) + b3
	y = identity_function(a3)

	return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)
```

- `forward` : ì‹ í˜¸ê°€ ìˆœë°©í–¥(ì…ë ¥ì—ì„œ ì¶œë ¥ ë°©í–¥)ìœ¼ë¡œ ì „ë‹¬ë¨(ìˆœì „íŒŒ)ì„ ì˜ë¯¸

---

## 3.5 ì¶œë ¥ì¸µ ì„¤ê³„í•˜ê¸°

- ì¼ë°˜ì ìœ¼ë¡œ íšŒê·€ì—ëŠ” í•­ë“±í•¨ìˆ˜ë¥¼, ë¶„ë¥˜ì—ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.

> ê¸°ê³„í•™ìŠµ ë¬¸ì œëŠ” **ë¶„ë¥˜**$^{\text{classification}}$ì™€ **íšŒê·€**$^{\text{regression}}$ë¡œ ë‚˜ë‰œë‹¤.

### 3.5.1 í•­ë“±í•¨ìˆ˜ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ êµ¬í˜„í•˜ê¸°

##### í•­ë“± í•¨ìˆ˜(identity function)
{: .no_toc }

<div class="mermaid">
graph LR;
	a1((a1)); a2((a2)); a3((a3)); y1((y1)); y2((y2)); y3((y3));
	a1 --"Ïƒ()"--> y1;
	a2 --"Ïƒ()"--> y2;
	a3 --"Ïƒ()"--> y3;
</div>

- ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•œë‹¤.

##### ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜(softmax function)
{: .no_toc }

- ë¶„ë¥˜ì—ì„œ ì‚¬ìš©í•œë‹¤.

<div>
$$
y_k = {exp(a_k) \over \sum_{i=1}^n exp(a_i)}
$$
</div>

- $exp(x)$ëŠ” $e^x$ë¥¼ ëœ»í•˜ëŠ” ì§€ìˆ˜ í•¨ìˆ˜(exponential function)ì´ë‹¤.

- $a_k$ëŠ” $k$ë²ˆì§¸ ì…ë ¥ ì‹ í˜¸, $n$ì€ ì¶œë ¥ì¸µì˜ ë‰´ëŸ°ìˆ˜, $y_k$ëŠ” $k$ë²ˆì§¸ ì¶œë ¥ì„ì„ ëœ»í•œë‹¤.

- ëª¨ë“  ì…ë ¥ ì‹ í˜¸ë¥¼ ë°›ì•„ ì¶œë ¥ì„ ê³„ì‚°í•œë‹¤.

<div class="mermaid">
graph LR;
	a1((a1)); a2((a2)); a3((a3)); y1((y1)); y2((y2)); y3((y3));
	a1 --"Ïƒ()"--> y1; a1 --"Ïƒ()"--> y2; a1 --"Ïƒ()"--> y3;
	a2 --"Ïƒ()"--> y1; a2 --"Ïƒ()"--> y2; a2 --"Ïƒ()"--> y3;
	a3 --"Ïƒ()"--> y1; a3 --"Ïƒ()"--> y2; a3 --"Ïƒ()"--> y3;
</div>

```python
a = np.array([0.3, 2.9, 4.0])
exp_a = np.exp(a)
print(exp_a)
sum_exp_a = np.sum(exp_a)
print(sum_exp_a)
y = exp_a / sum_exp_a
print(y)
```

```python
def softmax(a):
	exp_a = np.exp(a)
	sum_exp_a = np.sum(exp_a)
	y = exp_a / sum_exp_a

	return y
```

### 3.5.2 ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ êµ¬í˜„ ì‹œ ì£¼ì˜ì 

- ì§€ìˆ˜í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì˜¤ë²„í”Œë¡œ ì£¼ì˜

- ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë„ë¡ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ êµ¬í˜„ ê°œì„ 

<div>
$$
\begin{aligned}
y_k = {exp(a_k) \over \sum_{i=1}^nexp(a_i)} &= {Cexp(a_k) \over C\sum_{i=1}^nexp(a_i)} \\
	&= {exp(a_k + logC) \over \sum_{i=1}^nexp(a_i + logC)} \\
	&= {exp(a_k + C^\prime) \over \sum_{i=1}^nexp(a_i+C^\prime)}
\end{aligned}
$$
</div>

- ì†Œí”„íŠ¸ë§¥ìŠ¤ì˜ ì§€ìˆ˜ í•¨ìˆ˜ë¥¼ ê³„ì‚°í•  ë•Œ ì–´ë–¤ ì •ìˆ˜ë¥¼ ë”í•´ë„ (í˜¹ì€ ë¹¼ë„) ê²°ê³¼ëŠ” ë°”ë€Œì§€ ì•ŠëŠ”ë‹¤.

- ì˜¤ë²„í”Œë¡œë¥¼ ë§‰ì„ ëª©ì ìœ¼ë¡œ $C^\prime$ì— ì…ë ¥ ì‹ í˜¸ ì¤‘ $-ìµœëŒ“ê°’$ì„ ëŒ€ì…í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 

```python
# interpreter
a = np.array([1010, 1000, 990])
np.exp(a) / np.sum(np.exp(a))	# ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì˜ ê³„ì‚° ì œëŒ€ë¡œ ì•ˆ ë¨
c = np.max(a)
a - c
np.exp(a - c) / np.sum(np.exp(a - c))
```

```python
def softmax(a):
	c = np.max(a)
	exp_a = np.exp(a - c)	# ì˜¤ë²„í”Œë¡œ ëŒ€ì±…
	sum_exp_a = np.sum(exp_a)
	y = exp_a / sum_exp_a

	return y
```

### 3.5.3 ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì˜ íŠ¹ì§•

- ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì˜ ì¶œë ¥ì€ 0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì‹¤ìˆ˜

- ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ ì¶œë ¥ì˜ ì´í•©ì€ 1

- ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì˜ ì¶œë ¥ì„ í™•ë¥ ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤.

- ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì ìš©í•´ë„ ê° ì›ì†Œì˜ ëŒ€ì†Œ ê´€ê³„ëŠ” ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤.

- í˜„ì—…ì—ì„œ ì‹ ê²½ë§ì„ ì´ìš©í•œ ë¶„ë¥˜ë¥¼ í•  ë•Œ ì§€ìˆ˜ í•¨ìˆ˜ ê³„ì‚°ì— ë“œëŠ” ìì› ë‚­ë¹„ë¥¼ ì¤„ì´ê³ ì ì¶œë ¥ì¸µì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ëŠ” ìƒëµí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 

> ê¸°ê³„í•™ìŠµì˜ ë¬¸ì œ í’€ì´ëŠ” **í•™ìŠµ**ê³¼ **ì¶”ë¡ **$^{\text{inference}}$ì˜ ë‹¨ê³„ë¥¼ ê±°ì³ ì´ë£¨ì–´ì§„ë‹¤.
> í•™ìŠµ ë‹¨ê³„ì—ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì¶”ë¡  ë‹¨ê³„ì—ì„œ ì•ì„œ í•™ìŠµí•œ ëª¨ë¸ë¡œ ë¯¸ì§€ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œ ì¶”ë¡ (ë¶„ë¥˜)ë¥¼ ìˆ˜í–‰í•œë‹¤.
> ì¶”ë¡  ë‹¨ê³„ì—ì„œëŠ” ì¶œë ¥ì¸µì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ìƒëµí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‚˜, ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¬ ë•ŒëŠ” ì¶œë ¥ì¸µì—ì„œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.

---

## 3.6 ì†ê¸€ì”¨ ìˆ«ì ì¸ì‹

- ì´ë¯¸ í•™ìŠµëœ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ê³¼ì •ì€ ìƒëµí•˜ê³  ì¶”ë¡  ê³¼ì •ë§Œ êµ¬í˜„

- ì´ ì¶”ë¡  ê³¼ì •ì„ ì‹ ê²½ë§ì˜ **ìˆœì „íŒŒ**$^{\text{forward propagation}}$ë¼ê³ ë„ í•œë‹¤.

> ê¸°ê³„í•™ìŠµê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì‹ ê²½ë§ë„ í›ˆë ¨ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ í•™ìŠµí•˜ê³ , ì¶”ë¡  ë‹¨ê³„ì—ì„œëŠ” ì•ì„œ í•™ìŠµí•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•œë‹¤.

### 3.6.1 MNIST ë°ì´í„°ì…‹

- ê¸°ê³„í•™ìŠµ ë¶„ì•¼ì—ì„œ ì•„ì£¼ ìœ ëª…í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ê°„ë‹¨í•œ ì‹¤í—˜ë¶€í„° ë…¼ë¬¸ìœ¼ë¡œ ë°œí‘œë˜ëŠ” ì—°êµ¬ê¹Œì§€ ë‹¤ì–‘í•œ ê³³ì—ì„œ ì´ìš©ë¨

- 0ë¶€í„° 9ê¹Œì§€ì˜ ìˆ«ì ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœë‹¤.

- í›ˆë ¨ ì´ë¯¸ì§€ 60,000ì¥, ì‹œí—˜ ì´ë¯¸ì§€ 10,000ì¥

- MNISTì˜ ì´ë¯¸ì§€ ë°ì´í„°ëŠ” $28 \times 28$ í¬ê¸°ì˜ íšŒìƒ‰ì¡° ì´ë¯¸ì§€(1ì±„ë„)ì´ë©°, ê° í”½ì…€ì€ 0ì—ì„œ 255ê¹Œì§€ì˜ ê°’ì„ ì·¨í•œë‹¤.

- ê° ì´ë¯¸ì§€ì—ëŠ” ì´ë¯¸ì§€ê°€ ì‹¤ì œ ì˜ë¯¸í•˜ëŠ” ìˆ«ìê°€ ë ˆì´ë¸”ë¡œ ë¶™ì–´ ìˆë‹¤.

```python
# import sys, os
# sys.path.append(os.pardir)	# ë¶€ëª¨ ë””ë ‰í„°ë¦¬ì˜ íŒŒì¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ ì„¤ì •
from dataset.mnist import load_mnist

# ì²˜ìŒ í•œ ë²ˆì€ ëª‡ ë¶„ ì •ë„ ê±¸ë¦¼
(x_train, t_train), (x_test, t_test) = \
	load_mnist(flatten=True, normalize=False)

# ê° ë°ì´í„°ì˜ í˜•ìƒ ì¶œë ¥
print(x_train.shape)
print(t_train.shape)
print(x_test.shape)
print(t_test.shape)
```

- MNIST ë°ì´í„°ì…‹ì„ ë‚´ë ¤ë°›ì•„ ì´ë¯¸ì§€ë¥¼ ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜í•´ì£¼ëŠ” íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸

> `ğŸ“mnist.py`ê°€ `ğŸ“dataset`ì—  ìˆë‹¤ê³  ê°€ì •í•œ ìŠ¤í¬ë¦½íŠ¸

- `load_mnist`ê°€ MNIST ë°ì´í„°ë¥¼ ë°›ì•„ì™€ì•¼ í•˜ë‹ˆ ìµœì´ˆ ì‹¤í–‰ ì‹œì—ëŠ” ì¸í„°ë„·ì— ì—°ê²°ëœ ìƒíƒœì—¬ì•¼ í•œë‹¤. ë‘ ë²ˆì§¸ë¶€í„°ëŠ” ë¡œì»¬ì— ì €ì¥ëœ íŒŒì¼(pickle íŒŒì¼)ì„ ì½ê¸° ë•Œë¬¸ì— ë¹¨ë¦¬ ëë‚œë‹¤.

##### load_mnist
{: .no_toc }

###### return
{: .no_toc }

- ì½ì€ MNIST ë°ì´í„°ë¥¼ `(í›ˆë ¨ ì´ë¯¸ì§€, í›ˆë ¨ ë ˆì´ë¸”), (ì‹œí—˜ ì´ë¯¸ì§€, ì‹œí—˜ ë ˆì´ë¸”)` í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.

- ì´ë¯¸ì§€, ë ˆì´ë¸” ê°ê°ì€ `ndarray`

###### arguments
{: .no_toc }

- `normalize` : ì…ë ¥ ì´ë¯¸ì§€ì˜ í”½ì…€ê°’ì„ 0.0~1.0 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì •ê·œí™”í• ì§€ ì •í•œë‹¤.

	- `False`ë¡œ ì„¤ì •í•˜ë©´ ì…ë ¥ ì´ë¯¸ì§€ í”½ì…€ê°’ ê·¸ëŒ€ë¡œ 0~255 ì‚¬ì´ì˜ ê°’ ìœ ì§€í•˜ê³  `True`ë©´ 255ë¡œ ë‚˜ëˆ  0.0~1.0 ë²”ìœ„ë¡œ ë³€í™˜í•œë‹¤.

- `flatten` : ì…ë ¥ì´ë¯¸ì§€ë¥¼ 1ì°¨ì› ë°°ì—´ë¡œ ë§Œë“¤ì§€ ì •í•œë‹¤.

	- `False`ë¡œ ì„¤ì •í•˜ë©´ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ $1 \times 28 \times 28$ì˜ 3ì°¨ì› ë°°ì—´ë¡œ, `True`ë¡œ ì„¤ì •í•˜ë©´ 784ê°œì˜ ì›ì†Œë¡œ ì´ë£¨ì–´ì§„ 1ì°¨ì› ë°°ì—´ë¡œ ì €ì¥

- `one_hot_label` : ë ˆì´ë¸”ì„ **one-hot encoding** í˜•íƒœë¡œ ì €ì¥í• ì§€ ì •í•œë‹¤.

	- one-hot encoding : ì •ë‹µì„ ëœ»í•˜ëŠ” ì›ì†Œë§Œ 1(hot)ì´ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ì¸ ë°°ì—´ë¡œ ì €ì¥

	- `False`ë©´ ìˆ«ì í˜•íƒœì˜ ë ˆì´ë¸”ì„ ì €ì¥í•˜ê³  `True`ì¼ ë•ŒëŠ” ë ˆì´ë¸”ì„ ì›-í•« ì¸ì½”ë”©í•˜ì—¬ ì €ì¥

> íŒŒì´ì¬ì—ëŠ” pickle(í”¼í´)ì´ë¼ëŠ” ê¸°ëŠ¥ì´ ìˆë‹¤.
> ì´ëŠ” í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ì— íŠ¹ì • ê°ì²´ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì´ë‹¤.
> ì €ì¥í•´ë‘” pickle íŒŒì¼ì„ ë¡œë“œí•˜ë©´ ì‹¤í–‰ ë‹¹ì‹œì˜ ê°ì²´ë¥¼ ì¦‰ì‹œ ë³µì›í•  ìˆ˜ ìˆë‹¤.
> MNIST ë°ì´í„°ì…‹ë¥´ ì½ëŠ” `load_mnist` í•¨ìˆ˜ì—ì„œë„ (2ë²ˆì§¸ ì´í›„ì˜ ì½ê¸° ì‹œ) pickleì„ ì´ìš©í•œë‹¤.

```python
import numpy as np
from dataset.mnist import load_mnist
from PIL import Image	# ì´ë¯¸ì§€ í‘œì‹œì— ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆ PIL(Python Image Library)

def img_show(img):
	pil_img = Image.fromarray(np.uint8(img))
	pil_img.show()

(x_train, t_train), (x_test, t_test) = \
	load_mnist(flatten=True, normalize=False)

img = x_train[0]
label = t_train[0]
print(label)

print(img.shape)
img = img.reshape(28, 28) # ì›ë˜ ì´ë¯¸ì§€ì˜ ëª¨ì–‘ìœ¼ë¡œ ë³€í˜•
print(img.shape)

img_show(img)
```

- `flatten=True`ë¡œ ì„¤ì •í•´ ì´ë¯¸ì§€ë¥¼ ì½ì–´ë“¤ì˜€ê¸° ë•Œë¬¸ì— ì´ë¯¸ì§€ë¥¼ í‘œì‹œí•  ë•ŒëŠ” ì›ë˜ í˜•ìƒì¸ $28 \times 28$ í¬ê¸°ë¡œ ë‹¤ì‹œ ë³€í˜•í•´ì•¼ í•œë‹¤.

- `ndarray.reshape()` : ì›í•˜ëŠ” í˜•ìƒì„ ì¸ìˆ˜ë¡œ ì§€ì •í•˜ë©´ ë„˜íŒŒì´ ë°°ì—´ì˜ í˜•ìƒì„ ë°”ê¿€ ìˆ˜ ìˆë‹¤.

- `Image.fromarray` : ë„˜íŒŒì´ë¡œ ì €ì¥ëœ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ PILìš© ë°ì´í„° ê°ì²´ë¡œ ë³€í™˜

### 3.6.2 ì‹ ê²½ë§ì˜ ì¶”ë¡  ì²˜ë¦¬

- ì…ë ¥ì¸µ ë‰´ëŸ° 784ê°œ(ì´ë¯¸ì§€ í¬ê¸° $28 \times 28$), ì¶œë ¥ì¸µ ë‰´ëŸ° 10ê°œ(0~9 ìˆ«ì êµ¬ë¶„)

- ì€ë‹‰ì¸µì€ ì´ ë‘ ê°œ

	- ì„ì˜ë¡œ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì—ëŠ” 50ê°œì˜ ë‰´ëŸ°ì„, ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µì—ëŠ” 100ê°œì˜ ë‰´ëŸ°ì„ ë°°ì¹˜í•  ê²ƒì´ë‹¤.

- github `WegraLee/deep-learning-from-scratch`ì˜ `dataset/mnist.py`ë¥¼ ì‚¬ìš©

```python
import numpy as np
import pickle
from dataset.mnist import load_mnist
from common.functions import sigmoid, softmax


def get_data():
	(x_train, t_train), (x_test, t_test) = \
		load_mnist(normalize=True, flatten=True, one_hot_label=False)
	return x_test, t_test

def init_network():
	with open("sample_weight.pkl", 'rb') as f:
		network = pickle.load(f)
	return network

def predict(network, x):
	W1, W2, W3 = network['W1'], network['W2'], network['W3']
	b1, b2, b3 = network['b1'], network['b2'], network['b3']

	a1 = np.dot(x, W1) + b1
	z1 = sigmoid(a1)
	a2 = np.dot(z1, W2) + b2
	z2 = sigmoid(a2)
	a3 = np.dot(z2, W3) + b3
	y = softmax(a3)

	return y

# ì •í™•ë„(accuracy) í‰ê°€
x, t = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
	y = predict(network, x[i])
	p = np.argmax(y)	# í™•ë¥ ì´ ê°€ì¥ ë†“ì€ ì›ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ì–»ëŠ”ë‹¤.
	if p == t[i]:
		accuracy_cnt += 1

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```

- `init_network` : pickle íŒŒì¼ì¸ `ğŸ“sample_weight.pkl`ì— ì €ì¥ëœ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì½ëŠ”ë‹¤.

	- github `WegraLee/deep-learning-from-scratch`ì˜ `ch03/sample_weight.pkl` : ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ë§¤ê°œë³€ìˆ˜ê°€ ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜ë¡œ ì €ì¥ë˜ì–´ ìˆë‹¤.

- ë°ì´í„°ë¥¼ íŠ¹ì • ë²”ìœ„ë¡œ ë³€í™˜í•˜ëŠ” ì²˜ë¦¬ë¥¼ **ì •ê·œí™”**$^{\text{normalization}}$ë¼ê³  í•˜ê³ , ì‹ ê²½ë§ì˜ ì…ë ¥ ë°ì´í„°ì— íŠ¹ì • ë³€í™˜ì„ ê°€í•˜ëŠ” ê²ƒì„ **ì „ì²˜ë¦¬**$^{\text{pre-processing}}$ë¼ê³  í•œë‹¤.

> í˜„ì—…ì—ì„œë„ ì‹ ê²½ë§(ë”¥ëŸ¬ë‹)ì— ì „ì²˜ë¦¬ë¥¼ í™œë°œíˆ ì‚¬ìš©í•œë‹¤. ì „ì²˜ë¦¬ë¥¼ í†µí•´ ì‹ë³„ ëŠ¥ë ¥ì„ ê°œì„ í•˜ê³  í•™ìŠµ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤. ì•ì˜ ì˜ˆì—ì„œëŠ” ê° í”½ì…€ì˜ ê°’ì„ 255ë¡œ ë‚˜ëˆ„ëŠ” ë‹¨ìˆœí•œ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í–ˆì§€ë§Œ, í˜„ì—…ì—ì„œëŠ” ë°ì´í„° ì „ì²´ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•´ ì „ì²˜ë¦¬í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì—ì´í„° ì „ì²´ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ë“¤ì´ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„í¬í•˜ë„ë¡ ì´ë™í•˜ê±°ë‚˜ ë°ì´í„°ì˜ í™•ì‚° ë²”ìœ„ë¥¼ ì œí•œí•˜ëŠ” ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•œë‹¤. ê·¸ ì™¸ì—ë„ ì „ì²´ ë°ì´í„°ë¥¼ ê· ì¼í•˜ê²Œ ë¶„í¬ì‹œí‚¤ëŠ” ë°ì´í„° **ë°±ìƒ‰í™”**$^{\text{whitening}}$ ë“±ë„ ìˆë‹¤.

### 3.6.3 ë°°ì¹˜ ì²˜ë¦¬

- ì…ë ¥ ë°ì´í„°ì™€ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ í˜•ìƒì— ì£¼ì˜í•´ì„œ ì•ì˜ êµ¬í˜„ ë‹¤ì‹œ ì‚´í´ë³´ê¸°

```python
# interpreter
x, _ = get_data()
network = init_network()
W1, W2, W3 = network['W1'], network['W2'], network['W3']
x.shape
x[0].shape
W1.shape
W2.shape
W3.shape
```

<div>
$$
X~~~~~~~~~~~~~~W1~~~~~~~~~~~~~~W2~~~~~~~~~~~~~~W3~~~~\rightarrow~~~~Y
$$
$$
{\color{red}\cancel{784}~~~\cancel{784}} \times {\color{green}\cancel{50}~~~\cancel{50}} \times {\color{blue}\cancel{100}~~~\cancel{100}} \times \color{black}{10~~~~~~~~~10}
$$
</div>

- ì´ë¯¸ì§€ 1ì¥ ì…ë ¥í–ˆì„ ë•Œ íë¦„

<div>
$$
X~~~~~~~~~~~~~~~~W1~~~~~~~~~~~~~~~~W2~~~~~~~~~~~~~~~~W3~~~~~~\rightarrow~~~~~~Y
$$
$$
100 \times {\color{red}\cancel{784}~~~\cancel{784}} \times {\color{green}\cancel{50}~~~\cancel{50}} \times {\color{blue}\cancel{100}~~~\cancel{100}} \times \color{black}{10~~~~~~~~~~100 \times 10}
$$
</div>

- ì´ë¯¸ì§€ 100ì¥ ì…ë ¥í–ˆì„ ë•Œ íë¦„

- í•˜ë‚˜ë¡œ ë¬¶ì€ ì…ë ¥ ë°ì´í„°ë¥¼ **ë°°ì¹˜**$^{\text{batch}}$ë¼ê³  í•œë‹¤.

> ë°°ì¹˜ ì²˜ë¦¬ëŠ” ì´ë¯¸ì§€ 1ì¥ë‹¹ ì²˜ë¦¬ ì‹œê°„ì„ ëŒ€í­ ì¤„ì—¬ì¤€ë‹¤. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨ í° ë°°ì—´ë¡œ ì´ë¤„ì§„ ê³„ì‚°ì„ í•˜ê²Œ ë˜ëŠ”ë°, ì»´í“¨í„°ì—ì„œëŠ” í° ë°°ì—´ì„ í•œêº¼ë²ˆì— ê³„ì‚°í•˜ëŠ” ê²ƒì´ ë¶„í• ëœ ì‘ì€ ë°°ì—´ì„ ì—¬ëŸ¬ ë²ˆ ê³„ì‚°í•˜ëŠ” ê²ƒë³´ë‹¤ ë¹ ë¥´ë‹¤.
> ##### ë°°ì¹˜ ì²˜ë¦¬ê°€ ë” ë¹ ë¥¸ ì´ìœ 
> {: .no_toc }
> - ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ë¶€ë¶„ì´ í° ë°°ì—´ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ê³ ë„ë¡œ ìµœì í™”ë˜ì–´ ìˆë‹¤.
> - ì»¤ë‹¤ë€ ì‹ ê²½ë§ì—ì„œëŠ” ë°ì´í„° ì „ì†¡ì´ ë³‘ëª©ìœ¼ë¡œ ì‘ìš©í•˜ëŠ” ê²½ìš°ê°€ ìì£¼ ìˆëŠ”ë°, ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í•¨ìœ¼ë¡œì¨ ë²„ìŠ¤ì— ì£¼ëŠ” ë¶€í•˜ë¥¼ ì¤„ì¸ë‹¤.
>	- ë³‘ëª©(bottleneck) í˜„ìƒ: ì „ì²´ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì´ë‚˜ ìš©ëŸ‰ì´ í•˜ë‚˜ì˜ êµ¬ì„±ìš”ì†Œë¡œ ì¸í•´ ì œí•œì„ ë°›ëŠ” í˜„ìƒ
>	- ëŠë¦° I/Oë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì½ëŠ” íšŸìˆ˜ê°€ ì¤„ì–´ ë¹ ë¥¸ CPUë‚˜ GPUë¡œ ìˆœìˆ˜ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë¹„ìœ¨ì´ ë†’ì•„ì§„ë‹¤.

```python
import numpy as np
import pickle
from dataset.mnist import load_mnist
from common.functions import sigmoid, softmax


def get_data():
	(x_train, t_train), (x_test, t_test) = \
		load_mnist(normalize=True, flatten=True, one_hot_label=False)
	return x_test, t_test

def init_network():
	with open("sample_weight.pkl", 'rb') as f:
		network = pickle.load(f)
	return network

def predict(network, x):
	W1, W2, W3 = network['W1'], network['W2'], network['W3']
	b1, b2, b3 = network['b1'], network['b2'], network['b3']

	a1 = np.dot(x, W1) + b1
	z1 = sigmoid(a1)
	a2 = np.dot(z1, W2) + b2
	z2 = sigmoid(a2)
	a3 = np.dot(z2, W3) + b3
	y = softmax(a3)

	return y

x, t = get_data
network = init_network()

batch_size = 100 # ë°°ì¹˜ í¬ê¸°
accuracy_cnt = 0

for i in range(0, len(x), batch_size):
	x_batch = x[i:i+batch_size]
	y_batch = predict(network, x_batch)
	p = np.argmax(y_batch, axis=1)
	accuracy_cnt += np.sum(p == t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```

- `ndarray.argmax`ì˜ argument `axis` : ì§€ì •ëœ ì°¨ì›ì˜ ìµœëŒ“ê°’ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤.

##### ë™ì‘ í™•ì¸
{: .no_toc }

```python
# interpreter
x = np.array([[0.1, 0.8, 0.1], [0.3, 0.1, 0.6], [0.2, 0.5, 0.3], [0.8, 0.1, 0.1]])
y = np.argmax(x, axis=1)
print(y)
```

```python
# interpreter
y = np.array([1, 2, 1, 0])
t = np.array([1, 2, 0, 0])
print(y==t)
np.sum(y==t)
```

---

## 3.7 ì •ë¦¬

### ì´ë²ˆ ì¥ì—ì„œ ë°°ìš´ ë‚´ìš©

- ì‹ ê²½ë§ì—ì„œëŠ” í™œì„±í™” í•¨ìˆ˜ë¡œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ ReLU í•¨ìˆ˜ ê°™ì€ ë§¤ë„ëŸ½ê²Œ ë³€í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì´ìš©í•œë‹¤.

- ë„˜íŒŒì´ì˜ ë‹¤ì°¨ì› ë°°ì—´ì„ ì˜ ì‚¬ìš©í•˜ë©´ ì‹ ê²½ë§ì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

- ê¸°ê³„í•™ìŠµì˜ ë¬¸ì œëŠ” í¬ê²Œ íšŒê·€ì™€ ë¶„ë¥˜ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

- ì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” íšŒê·€ì—ì„œëŠ” ì£¼ë¡œ í•­ë“± í•¨ìˆ˜ë¥¼, ë¶„ë¥˜ì—ì„œëŠ” ì£¼ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì´ìš©í•œë‹¤.

- ë¶„ë¥˜ì—ì„œëŠ” ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ìˆ˜ë¥¼ ë¶„ë¥˜í•˜ë ¤ëŠ” í´ë˜ìŠ¤ ìˆ˜ì™€ ê°™ê²Œ ì„¤ì •í•œë‹¤.

- ì…ë ¥ ë°ì´í„°ë¥¼ ë¬¶ì€ ê²ƒì„ ë°°ì¹˜ë¼ í•˜ë©°, ì¶”ë¡  ì²˜ë¦¬ë¥¼ ì´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì§„í–‰í•˜ë©´ ê²°ê³¼ë¥¼ í›¨ì”¬ ë¹ ë¥´ê²Œ ì–»ì„ ìˆ˜ ìˆë‹¤.
